{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample script for AI moderator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - atproto\n",
    "\n",
    "- langchain           0.1.11\n",
    "- langchain-openai    0.0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/FujitsuResearch/atproto-python.git\n",
    "!pip install -U langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/FujitsuResearch/sample-moderator.git\n",
    "%cd sample-moderator/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3a319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atproto import Client\n",
    "\n",
    "handle = \"<your handle>\"\n",
    "password = \"<your password>\"\n",
    "channel = \"<Your channel>\"\n",
    "api_url = \"https://federated-sns-server[xx].research.global.fujitsu.com/xrpc\"\n",
    "token = \"<Your Azure AD token>\"\n",
    "prompt_file = \"input/test-prompt1.txt\" # sample prompt file\n",
    "client = Client(base_url=api_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee63329",
   "metadata": {},
   "source": [
    "### Setup - langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da3b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker python:3.12-bookworm for OpenAI\n",
    "import os \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<set OpenAI api key>\"\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",\n",
    "                  max_tokens=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23a46b",
   "metadata": {},
   "source": [
    "### Get posts and run LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22621fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "with open(prompt_file) as f:\n",
    "    template = f.read()\n",
    "    print(template)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables = [\"post\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-in\n",
    "try:\n",
    "    profile = client.login(handle, password, portal_token=token)\n",
    "    print(f\"Successfully logged in: {profile.handle}\")\n",
    "    \n",
    "except Exception as e :\n",
    "    print(e)\n",
    "\n",
    "# List joined channels\n",
    "response = client.app.fujitsu.channel.list_channel_info()\n",
    "joined_channel_list = [cinfo.channel_handle for cinfo in response.channel_info if channel in cinfo.channel_handle ]\n",
    "\n",
    "channel_name = joined_channel_list[0]\n",
    "print(\"Selected channel: \", channel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15482075",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 5\n",
    "flag = True\n",
    "cursor = None\n",
    "while flag :\n",
    "    response = client.get_channel_records(\n",
    "                    channel=channel_name,\n",
    "                    cursor=cursor,\n",
    "                    limit=limit,\n",
    "                )\n",
    "    if len(response.records) < limit :\n",
    "        flag = False\n",
    "    \n",
    "    cursor = response.cursor\n",
    "    \n",
    "    for record in response.records :\n",
    "        value = record.value.model_dump()\n",
    "        handle = value['handle']\n",
    "        text = value['text']\n",
    "        \n",
    "        formatted_prompt = prompt.format(post=text)\n",
    "        messages = [HumanMessage(content=formatted_prompt)]\n",
    "        result = chat.invoke(messages).content\n",
    "        \n",
    "        print(formatted_prompt)\n",
    "        print(result)\n",
    "        print(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
